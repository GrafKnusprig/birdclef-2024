{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70203,"databundleVersionId":8068726,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!rm -r /kaggle/working/*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install numpy tqdm librosa optuna timm pandas pillow","metadata":{"execution":{"iopub.status.busy":"2024-07-16T15:31:26.301446Z","iopub.execute_input":"2024-07-16T15:31:26.301842Z","iopub.status.idle":"2024-07-16T15:32:00.992532Z","shell.execute_reply.started":"2024-07-16T15:31:26.301783Z","shell.execute_reply":"2024-07-16T15:32:00.991453Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (0.10.2.post1)\nRequirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (3.6.1)\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (1.0.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (9.5.0)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa) (3.0.1)\nRequirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.11.4)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (5.1.1)\nRequirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.58.1)\nRequirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.12.1)\nRequirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.8.1)\nRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.3.7)\nRequirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (4.9.0)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.3)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.0.7)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.1)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.25)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from timm) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.2)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.23.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.41.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.1.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (3.11.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.32.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.3.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->timm) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.1.2)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install '/kaggle/input/noisereduce3/noisereduce-3.0.2-py3-none-any.whl'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # This create a small subset for testing\n\n# import os\n# import shutil\n# import sys\n\n# def copy_files(src_root, dest_root):\n#     # Create the destination root directory if it doesn't exist\n#     os.makedirs(dest_root, exist_ok=True)\n\n#     # Get the first 10 subdirectories\n#     subdirs = [os.path.join(src_root, d) for d in os.listdir(src_root) if os.path.isdir(os.path.join(src_root, d))]\n#     subdirs = subdirs[:5]\n\n#     for subdir in subdirs:\n#         # Get the first 5 files in the current subdirectory\n#         files = [os.path.join(subdir, f) for f in os.listdir(subdir) if os.path.isfile(os.path.join(subdir, f))]\n#         files = files[:5]\n\n#         # Create the corresponding subdirectory in the destination\n#         dest_subdir = os.path.join(dest_root, os.path.basename(subdir))\n#         os.makedirs(dest_subdir, exist_ok=True)\n\n#         for file in files:\n#             shutil.copy(file, dest_subdir)\n    \n#     print(\"Copying completed.\")\n\n# src_root = '/kaggle/input/birdclef-2024/train_audio'\n# dest_root = '/kaggle/working/train_audio_subset'\n\n# copy_files(src_root, dest_root)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import multiprocessing\n# import os\n# import time\n# from pathlib import Path\n# from io import BytesIO\n# from PIL import Image\n# import pandas as pd\n\n# import librosa\n# import matplotlib.pyplot as plt\n# import noisereduce as nr\n# import numpy as np\n# from tqdm import tqdm\n\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torchvision import transforms, datasets\n# from torch.utils.data import DataLoader, random_split\n# import timm\n# import optuna\n# from sklearn.metrics import roc_auc_score\n\n# _input_folder = \"/kaggle/input/birdclef-2024/train_audio\"\n# # _input_folder = '/kaggle/working/train_audio_subset' # For testing only\n# _test_folder = \"/kaggle/input/birdclef-2024/test_soundscapes\"\n# _output_model_folder = \"/kaggle/working/models\"\n# _output_log_folder = \"/kaggle/working/logs\"\n# _output_submission_folder = \"/kaggle/working\"\n# _sample_rate = 16000\n# _n_epochs = 5\n# _n_optuna_trials = 3\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# def load_audio(filename):\n#     audio, sr = librosa.load(filename, sr=_sample_rate)\n#     return audio, sr\n\n# def segment_audio(segment, segment_length=5, sr=_sample_rate):\n#     segmented_chunks = []\n#     samples_per_segment = segment_length * sr\n#     for start in range(0, len(segment), samples_per_segment):\n#         end = start + samples_per_segment\n#         segmented_chunks.append(segment[start:end])\n#     return segmented_chunks\n\n# def generate_square_spectrogram(audio, sr, size=224, fmin=2000, fmax=8000):\n#     s = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=fmax, fmin=fmin)\n#     s_dB = librosa.power_to_db(s, ref=np.max)\n#     fig, ax = plt.subplots(figsize=(size / 100, size / 100), dpi=100)\n#     img = librosa.display.specshow(s_dB, sr=sr, x_axis='time', y_axis='mel', fmin=fmin, fmax=fmax, cmap='gray', ax=ax)\n#     ax.axis('off')\n#     plt.tight_layout(pad=0)\n#     buf = BytesIO()\n#     fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n#     plt.close(fig)\n#     buf.seek(0)\n#     image = Image.open(buf).convert('RGB')\n#     return image\n\n# class CustomDataset(torch.utils.data.Dataset):\n#     def __init__(self, file_pairs, transform=None):\n#         self.file_pairs = file_pairs\n#         self.transform = transform\n\n#     def __len__(self):\n#         return sum(len(segment) for _, _, segment in self.file_pairs)\n\n#     def __getitem__(self, idx):\n#         current_idx = 0\n#         for input_file_path, label, segments in self.file_pairs:\n#             if idx < current_idx + len(segments):\n#                 segment = segments[idx - current_idx]\n#                 image = generate_square_spectrogram(segment, _sample_rate)\n#                 if self.transform:\n#                     image = self.transform(image)\n#                 return image, label\n#             current_idx += len(segments)\n#         raise IndexError(\"Index out of range\")\n\n# def prepare_file_pairs(input_folder):\n#     file_pairs = []\n#     input_folder = Path(input_folder)\n#     class_names = sorted([f.name for f in input_folder.iterdir() if f.is_dir()])\n#     class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n    \n#     for input_path in input_folder.rglob('*.ogg'):\n#         label = class_to_idx[input_path.parent.name]\n#         audio, sr = load_audio(input_path)\n#         audio = nr.reduce_noise(audio, sr)\n#         segments = segment_audio(audio, segment_length=5, sr=sr)\n#         file_pairs.append((input_path, label, segments))\n#     return file_pairs, len(class_names)\n\n# def objective(trial):\n#     model = timm.create_model('resnext50_32x4d', pretrained=True)\n\n#     transformation = transforms.Compose([\n#         transforms.Resize((224, 224)),\n#         transforms.ToTensor(),\n#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n#     ])\n\n#     file_pairs, num_classes = prepare_file_pairs(_input_folder)\n#     dataset = CustomDataset(file_pairs, transform=transformation)\n\n#     train_size = int(0.8 * len(dataset))\n#     val_size = len(dataset) - train_size\n#     train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n#     batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n#     val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n\n#     model.fc = nn.Linear(model.fc.in_features, num_classes)\n\n#     model.to(device)\n\n#     criterion = nn.CrossEntropyLoss()\n#     optimizer = optim.Adam(model.parameters(), lr=trial.suggest_loguniform(\"lr\", 1e-5, 1e-1))\n\n#     train_losses = []\n#     val_losses = []\n#     train_accuracies = []\n#     val_accuracies = []\n#     val_roc_aucs = []\n\n#     for epoch in range(_n_epochs):\n#         model.train()\n#         running_loss = 0.0\n#         correct_train = 0\n#         total_train = 0\n\n#         train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{_n_epochs}\")\n\n#         for inputs, labels in train_loader_tqdm:\n#             inputs, labels = inputs.to(device), labels.to(device)\n#             optimizer.zero_grad()\n#             outputs = model(inputs)\n#             loss = criterion(outputs, labels)\n#             loss.backward()\n#             optimizer.step()\n\n#             running_loss += loss.item() * inputs.size(0)\n#             _, predicted = torch.max(outputs, 1)\n#             total_train += labels.size(0)\n#             correct_train += (predicted == labels).sum().item()\n\n#             train_loader_tqdm.set_postfix(loss=loss.item())\n\n#         epoch_loss = running_loss / len(train_loader.dataset)\n#         epoch_accuracy = 100 * correct_train / total_train\n#         train_losses.append(epoch_loss)\n#         train_accuracies.append(epoch_accuracy)\n\n#         val_loss = 0.0\n#         correct_val = 0\n#         total_val = 0\n#         all_labels = []\n#         all_probs = []\n\n#         model.eval()\n#         val_loader_tqdm = tqdm(val_loader, desc=\"Validating\")\n\n#         with torch.no_grad():\n#             for inputs, labels in val_loader_tqdm:\n#                 inputs, labels = inputs.to(device), labels.to(device)\n#                 outputs = model(inputs)\n#                 loss = criterion(outputs, labels)\n#                 val_loss += loss.item() * inputs.size(0)\n#                 _, predicted = torch.max(outputs, 1)\n#                 total_val += labels.size(0)\n#                 correct_val += (predicted == labels).sum().item()\n\n#                 all_labels.extend(labels.cpu().numpy())\n#                 all_probs.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n\n#                 val_loader_tqdm.set_postfix(val_loss=loss.item())\n\n#         epoch_val_loss = val_loss / len(val_loader.dataset)\n#         epoch_val_accuracy = 100 * correct_val / total_val\n#         val_losses.append(epoch_val_loss)\n#         val_accuracies.append(epoch_val_accuracy)\n\n#         roc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n#         val_roc_aucs.append(roc_auc)\n\n#         trial.report(epoch_val_accuracy, epoch)\n\n#         if trial.should_prune():\n#             raise optuna.exceptions.TrialPruned()\n\n#     # Save the model after tuning\n#     os.makedirs(_output_model_folder, exist_ok=True)\n#     model_save_path = f\"{_output_model_folder}/model_best_resnext_trial_{trial.number}.pth\"\n#     torch.save(model.state_dict(), model_save_path)\n\n#     return val_accuracies[-1]\n\n# # Set up logging\n# os.makedirs(_output_log_folder, exist_ok=True)\n# log_file = f\"{_output_log_folder}/training_log.txt\"\n\n# def log_message(message):\n#     print(message)\n#     with open(log_file, 'a') as f:\n#         f.write(message + '\\n')\n\n# # Set up the Optuna study\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=_n_optuna_trials, timeout=600)\n\n# log_message(f\"Number of finished trials: {len(study.trials)}\")\n# log_message(\"Best trial:\")\n# trial = study.best_trial\n\n# log_message(f\"  Value: {trial.value}\")\n# log_message(\"  Params: \")\n# for key, value in trial.params.items():\n#     log_message(f\"    {key}: {value}\")\n\n# # Save the best model\n# file_pairs, num_classes = prepare_file_pairs(_input_folder)\n# best_model = timm.create_model('resnext50_32x4d', pretrained=True)\n# best_model.fc = nn.Linear(best_model.fc.in_features, num_classes)\n# best_model.load_state_dict(torch.load(f\"{_output_model_folder}/model_best_resnext_trial_{trial.number}.pth\"))\n# best_model_save_path = f\"{_output_model_folder}/model_best_resnext.pth\"\n# torch.save(best_model.state_dict(), best_model_save_path)\n\n# log_message(f\"Best model saved to {best_model_save_path}\")\n\n# # Load and test the best model\n# best_model.load_state_dict(torch.load(best_model_save_path))\n# best_model.eval()\n# best_model.to(device)\n\n# test_transformation = transforms.Compose([\n#     transforms.Resize((224, 224)),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n# ])\n\n# test_file_pairs, _ = prepare_file_pairs(_test_folder)\n# test_dataset = CustomDataset(test_file_pairs, transform=test_transformation)\n# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n# submission = []\n\n# with torch.no_grad():\n#     for idx, (inputs, _) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n#         inputs = inputs.to(device)\n#         outputs = best_model(inputs)\n#         probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy().flatten()\n        \n#         # Generate row_id\n#         row_id = os.path.basename(test_file_pairs[idx][0]).replace('.ogg', '')\n        \n#         submission.append([row_id] + probs.tolist())\n\n# # Create submission DataFrame\n# column_names = ['row_id'] + [f'label_{i}' for i in range(num_classes)]\n# submission_df = pd.DataFrame(submission, columns=column_names)\n\n# # Save to CSV\n# submission_csv_path = f\"{_output_submission_folder}/Submission.csv\"\n# submission_df.to_csv(submission_csv_path, index=False)\n# log_message(f\"Submission file saved to {submission_csv_path}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nfrom io import BytesIO\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport timm\nimport librosa\nimport librosa.display\nimport noisereduce as nr\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n_test_folder = \"/kaggle/input/birdclef-2024/test_soundscapes\"\n_output_submission_folder = \"/kaggle/working\"\n_sample_rate = 16000\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef load_audio(filename):\n    audio, sr = librosa.load(filename, sr=_sample_rate)\n    return audio, sr\n\ndef segment_audio(segment, segment_length=5, sr=_sample_rate):\n    segmented_chunks = []\n    samples_per_segment = segment_length * sr\n    for start in range(0, len(segment), samples_per_segment):\n        end = start + samples_per_segment\n        segmented_chunks.append(segment[start:end])\n    return segmented_chunks\n\ndef generate_square_spectrogram(audio, sr, size=224, fmin=2000, fmax=8000):\n    s = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=fmax, fmin=fmin)\n    s_dB = librosa.power_to_db(s, ref=np.max)\n    fig, ax = plt.subplots(figsize=(size / 100, size / 100), dpi=100)\n    img = librosa.display.specshow(s_dB, sr=sr, x_axis='time', y_axis='mel', fmin=fmin, fmax=fmax, cmap='gray', ax=ax)\n    ax.axis('off')\n    plt.tight_layout(pad=0)\n    buf = BytesIO()\n    fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n    plt.close(fig)\n    buf.seek(0)\n    image = Image.open(buf).convert('RGB')\n    return image\n\nclass CustomDataset(Dataset):\n    def __init__(self, file_pairs, transform=None):\n        self.file_pairs = file_pairs\n        self.transform = transform\n\n    def __len__(self):\n        return sum(len(segment) for _, _, segment in self.file_pairs)\n\n    def __getitem__(self, idx):\n        current_idx = 0\n        for input_file_path, label, segments in self.file_pairs:\n            if idx < current_idx + len(segments):\n                segment = segments[idx - current_idx]\n                image = generate_square_spectrogram(segment, _sample_rate)\n                if self.transform:\n                    image = self.transform(image)\n                return image, label\n            current_idx += len(segments)\n        raise IndexError(\"Index out of range\")\n\ndef prepare_file_pairs(input_folder):\n    file_pairs = []\n    input_folder = Path(input_folder)\n    class_names = sorted([f.name for f in input_folder.iterdir() if f.is_dir()])\n    class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n    \n    for input_path in tqdm(input_folder.rglob('*.ogg'), desc=\"Preparing file pairs\"):\n        label = class_to_idx[input_path.parent.name]\n        audio, sr = load_audio(input_path)\n        audio = nr.reduce_noise(audio, sr)\n        segments = segment_audio(audio, segment_length=5, sr=sr)\n        file_pairs.append((input_path, label, segments))\n    return file_pairs, len(class_names)\n\n# Define the number of classes manually or from the training data\nnum_classes = 182  # Replace with the actual number of classes\n\n# Load and test the best model\nmodel_path = '/kaggle/input/resnext_best/pytorch/resnext_best/1/model_best_resnext.pth'\nbest_model = timm.create_model('resnext50_32x4d', pretrained=True)\nbest_model.fc = nn.Linear(best_model.fc.in_features, num_classes)\nbest_model.load_state_dict(torch.load(model_path))\nbest_model.eval()\nbest_model.to(device)\n\ntest_transformation = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_file_pairs, _ = prepare_file_pairs(_test_folder)\ntest_dataset = CustomDataset(test_file_pairs, transform=test_transformation)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\nsubmission = []\n\nwith torch.no_grad():\n    for idx, (inputs, _) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n        inputs = inputs.to(device)\n        outputs = best_model(inputs)\n        probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy().flatten()\n        \n        # Generate row_id\n        row_id = os.path.basename(test_file_pairs[idx][0]).replace('.ogg', '')\n        \n        submission.append([row_id] + probs.tolist())\n\n# Create submission DataFrame\ncolumn_names = ['row_id'] + [f'label_{i}' for i in range(num_classes)]\nsubmission_df = pd.DataFrame(submission, columns=column_names)\n\n# Save to CSV\nsubmission_csv_path = f\"{_output_submission_folder}/Submission.csv\"\nsubmission_df.to_csv(submission_csv_path, index=False)\nprint(f\"Submission file saved to {submission_csv_path}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n\n# def print_directory_structure(root_dir, indent=\"\"):\n#     for item in os.listdir(root_dir):\n#         item_path = os.path.join(root_dir, item)\n#         if os.path.isdir(item_path):\n#             print(indent + \"|-- \" + item)\n#             print_directory_structure(item_path, indent + \"    \")\n#         elif not item.endswith('.ogg'):\n#             print(indent + \"|-- \" + item)\n\n# # Change '/kaggle/input' to the root directory of your Kaggle project folder\n# root_directory = '/kaggle/input'\n# print_directory_structure(root_directory)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n\n# train_audio_folder = '/kaggle/input/birdclef-2024/train_audio'\n\n# def count_folders(directory):\n#     return sum(1 for entry in os.scandir(directory) if entry.is_dir())\n\n# folder_count = count_folders(train_audio_folder)\n# print(f\"The number of folders in '{train_audio_folder}' is: {folder_count}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}