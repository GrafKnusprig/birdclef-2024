{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70203,"databundleVersionId":8068726,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!rm -r /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-07-16T14:09:27.369857Z","iopub.execute_input":"2024-07-16T14:09:27.370374Z","iopub.status.idle":"2024-07-16T14:09:28.558683Z","shell.execute_reply.started":"2024-07-16T14:09:27.370330Z","shell.execute_reply":"2024-07-16T14:09:28.556823Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"rm: cannot remove '/kaggle/working/*': No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"execution":{"iopub.status.busy":"2024-07-16T14:09:28.561791Z","iopub.execute_input":"2024-07-16T14:09:28.562268Z","iopub.status.idle":"2024-07-16T14:09:45.854913Z","shell.execute_reply.started":"2024-07-16T14:09:28.562228Z","shell.execute_reply":"2024-07-16T14:09:45.853102Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2+cpu)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install noisereduce numpy tqdm librosa optuna timm scikit-metrics pandas pillow","metadata":{"execution":{"iopub.status.busy":"2024-07-16T14:09:45.856933Z","iopub.execute_input":"2024-07-16T14:09:45.857474Z","iopub.status.idle":"2024-07-16T14:10:04.966558Z","shell.execute_reply.started":"2024-07-16T14:09:45.857370Z","shell.execute_reply":"2024-07-16T14:10:04.965016Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Collecting noisereduce\n  Downloading noisereduce-3.0.2-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (0.10.2.post1)\nRequirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (3.6.1)\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (1.0.3)\nCollecting scikit-metrics\n  Downloading scikit-metrics-0.1.0.tar.gz (3.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (9.5.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from noisereduce) (1.11.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from noisereduce) (3.7.5)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa) (3.0.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (5.1.1)\nRequirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.58.1)\nRequirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.12.1)\nRequirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.8.1)\nRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.3.7)\nRequirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (4.9.0)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.3)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.0.7)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.1)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.25)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from timm) (2.1.2+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.2+cpu)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.23.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.41.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.1.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (3.11.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.32.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.3.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->noisereduce) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->noisereduce) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->noisereduce) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->noisereduce) (1.4.5)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->timm) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.1.2)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\nDownloading noisereduce-3.0.2-py3-none-any.whl (22 kB)\nBuilding wheels for collected packages: scikit-metrics\n  Building wheel for scikit-metrics (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for scikit-metrics: filename=scikit_metrics-0.1.0-py3-none-any.whl size=4394 sha256=728c55b431844efa01dc8c8da9c7f051f7a851bc4e041c513ef8a2535e3b80c4\n  Stored in directory: /root/.cache/pip/wheels/45/a3/9b/8f375bbf235b0cfbee9a841c94b4bbfb13fad10db9cab4d5d4\nSuccessfully built scikit-metrics\nInstalling collected packages: scikit-metrics, noisereduce\nSuccessfully installed noisereduce-3.0.2 scikit-metrics-0.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# # This create a small subset for testing\n\n# import os\n# import shutil\n# import sys\n\n# def copy_files(src_root, dest_root):\n#     # Create the destination root directory if it doesn't exist\n#     os.makedirs(dest_root, exist_ok=True)\n\n#     # Get the first 10 subdirectories\n#     subdirs = [os.path.join(src_root, d) for d in os.listdir(src_root) if os.path.isdir(os.path.join(src_root, d))]\n#     subdirs = subdirs[:5]\n\n#     for subdir in subdirs:\n#         # Get the first 5 files in the current subdirectory\n#         files = [os.path.join(subdir, f) for f in os.listdir(subdir) if os.path.isfile(os.path.join(subdir, f))]\n#         files = files[:5]\n\n#         # Create the corresponding subdirectory in the destination\n#         dest_subdir = os.path.join(dest_root, os.path.basename(subdir))\n#         os.makedirs(dest_subdir, exist_ok=True)\n\n#         for file in files:\n#             shutil.copy(file, dest_subdir)\n    \n#     print(\"Copying completed.\")\n\n# src_root = '/kaggle/input/birdclef-2024/train_audio'\n# dest_root = '/kaggle/working/train_audio_subset'\n\n# copy_files(src_root, dest_root)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T14:10:04.970602Z","iopub.execute_input":"2024-07-16T14:10:04.971166Z","iopub.status.idle":"2024-07-16T14:10:04.978024Z","shell.execute_reply.started":"2024-07-16T14:10:04.971110Z","shell.execute_reply":"2024-07-16T14:10:04.976765Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# import multiprocessing\n# import os\n# import time\n# from pathlib import Path\n# from io import BytesIO\n# from PIL import Image\n# import pandas as pd\n\n# import librosa\n# import matplotlib.pyplot as plt\n# import noisereduce as nr\n# import numpy as np\n# from tqdm import tqdm\n\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torchvision import transforms, datasets\n# from torch.utils.data import DataLoader, random_split\n# import timm\n# import optuna\n# from sklearn.metrics import roc_auc_score\n\n# _input_folder = \"/kaggle/input/birdclef-2024/train_audio\"\n# # _input_folder = '/kaggle/working/train_audio_subset' # For testing only\n# _test_folder = \"/kaggle/input/birdclef-2024/test_soundscapes\"\n# _output_model_folder = \"/kaggle/working/models\"\n# _output_log_folder = \"/kaggle/working/logs\"\n# _output_submission_folder = \"/kaggle/working\"\n# _sample_rate = 16000\n# _n_epochs = 5\n# _n_optuna_trials = 3\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# def load_audio(filename):\n#     audio, sr = librosa.load(filename, sr=_sample_rate)\n#     return audio, sr\n\n# def segment_audio(segment, segment_length=5, sr=_sample_rate):\n#     segmented_chunks = []\n#     samples_per_segment = segment_length * sr\n#     for start in range(0, len(segment), samples_per_segment):\n#         end = start + samples_per_segment\n#         segmented_chunks.append(segment[start:end])\n#     return segmented_chunks\n\n# def generate_square_spectrogram(audio, sr, size=224, fmin=2000, fmax=8000):\n#     s = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=fmax, fmin=fmin)\n#     s_dB = librosa.power_to_db(s, ref=np.max)\n#     fig, ax = plt.subplots(figsize=(size / 100, size / 100), dpi=100)\n#     img = librosa.display.specshow(s_dB, sr=sr, x_axis='time', y_axis='mel', fmin=fmin, fmax=fmax, cmap='gray', ax=ax)\n#     ax.axis('off')\n#     plt.tight_layout(pad=0)\n#     buf = BytesIO()\n#     fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n#     plt.close(fig)\n#     buf.seek(0)\n#     image = Image.open(buf).convert('RGB')\n#     return image\n\n# class CustomDataset(torch.utils.data.Dataset):\n#     def __init__(self, file_pairs, transform=None):\n#         self.file_pairs = file_pairs\n#         self.transform = transform\n\n#     def __len__(self):\n#         return sum(len(segment) for _, _, segment in self.file_pairs)\n\n#     def __getitem__(self, idx):\n#         current_idx = 0\n#         for input_file_path, label, segments in self.file_pairs:\n#             if idx < current_idx + len(segments):\n#                 segment = segments[idx - current_idx]\n#                 image = generate_square_spectrogram(segment, _sample_rate)\n#                 if self.transform:\n#                     image = self.transform(image)\n#                 return image, label\n#             current_idx += len(segments)\n#         raise IndexError(\"Index out of range\")\n\n# def prepare_file_pairs(input_folder):\n#     file_pairs = []\n#     input_folder = Path(input_folder)\n#     class_names = sorted([f.name for f in input_folder.iterdir() if f.is_dir()])\n#     class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n    \n#     for input_path in input_folder.rglob('*.ogg'):\n#         label = class_to_idx[input_path.parent.name]\n#         audio, sr = load_audio(input_path)\n#         audio = nr.reduce_noise(audio, sr)\n#         segments = segment_audio(audio, segment_length=5, sr=sr)\n#         file_pairs.append((input_path, label, segments))\n#     return file_pairs, len(class_names)\n\n# def objective(trial):\n#     model = timm.create_model('resnext50_32x4d', pretrained=True)\n\n#     transformation = transforms.Compose([\n#         transforms.Resize((224, 224)),\n#         transforms.ToTensor(),\n#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n#     ])\n\n#     file_pairs, num_classes = prepare_file_pairs(_input_folder)\n#     dataset = CustomDataset(file_pairs, transform=transformation)\n\n#     train_size = int(0.8 * len(dataset))\n#     val_size = len(dataset) - train_size\n#     train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n#     batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n#     val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n\n#     model.fc = nn.Linear(model.fc.in_features, num_classes)\n\n#     model.to(device)\n\n#     criterion = nn.CrossEntropyLoss()\n#     optimizer = optim.Adam(model.parameters(), lr=trial.suggest_loguniform(\"lr\", 1e-5, 1e-1))\n\n#     train_losses = []\n#     val_losses = []\n#     train_accuracies = []\n#     val_accuracies = []\n#     val_roc_aucs = []\n\n#     for epoch in range(_n_epochs):\n#         model.train()\n#         running_loss = 0.0\n#         correct_train = 0\n#         total_train = 0\n\n#         train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{_n_epochs}\")\n\n#         for inputs, labels in train_loader_tqdm:\n#             inputs, labels = inputs.to(device), labels.to(device)\n#             optimizer.zero_grad()\n#             outputs = model(inputs)\n#             loss = criterion(outputs, labels)\n#             loss.backward()\n#             optimizer.step()\n\n#             running_loss += loss.item() * inputs.size(0)\n#             _, predicted = torch.max(outputs, 1)\n#             total_train += labels.size(0)\n#             correct_train += (predicted == labels).sum().item()\n\n#             train_loader_tqdm.set_postfix(loss=loss.item())\n\n#         epoch_loss = running_loss / len(train_loader.dataset)\n#         epoch_accuracy = 100 * correct_train / total_train\n#         train_losses.append(epoch_loss)\n#         train_accuracies.append(epoch_accuracy)\n\n#         val_loss = 0.0\n#         correct_val = 0\n#         total_val = 0\n#         all_labels = []\n#         all_probs = []\n\n#         model.eval()\n#         val_loader_tqdm = tqdm(val_loader, desc=\"Validating\")\n\n#         with torch.no_grad():\n#             for inputs, labels in val_loader_tqdm:\n#                 inputs, labels = inputs.to(device), labels.to(device)\n#                 outputs = model(inputs)\n#                 loss = criterion(outputs, labels)\n#                 val_loss += loss.item() * inputs.size(0)\n#                 _, predicted = torch.max(outputs, 1)\n#                 total_val += labels.size(0)\n#                 correct_val += (predicted == labels).sum().item()\n\n#                 all_labels.extend(labels.cpu().numpy())\n#                 all_probs.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n\n#                 val_loader_tqdm.set_postfix(val_loss=loss.item())\n\n#         epoch_val_loss = val_loss / len(val_loader.dataset)\n#         epoch_val_accuracy = 100 * correct_val / total_val\n#         val_losses.append(epoch_val_loss)\n#         val_accuracies.append(epoch_val_accuracy)\n\n#         roc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n#         val_roc_aucs.append(roc_auc)\n\n#         trial.report(epoch_val_accuracy, epoch)\n\n#         if trial.should_prune():\n#             raise optuna.exceptions.TrialPruned()\n\n#     # Save the model after tuning\n#     os.makedirs(_output_model_folder, exist_ok=True)\n#     model_save_path = f\"{_output_model_folder}/model_best_resnext_trial_{trial.number}.pth\"\n#     torch.save(model.state_dict(), model_save_path)\n\n#     return val_accuracies[-1]\n\n# # Set up logging\n# os.makedirs(_output_log_folder, exist_ok=True)\n# log_file = f\"{_output_log_folder}/training_log.txt\"\n\n# def log_message(message):\n#     print(message)\n#     with open(log_file, 'a') as f:\n#         f.write(message + '\\n')\n\n# # Set up the Optuna study\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=_n_optuna_trials, timeout=600)\n\n# log_message(f\"Number of finished trials: {len(study.trials)}\")\n# log_message(\"Best trial:\")\n# trial = study.best_trial\n\n# log_message(f\"  Value: {trial.value}\")\n# log_message(\"  Params: \")\n# for key, value in trial.params.items():\n#     log_message(f\"    {key}: {value}\")\n\n# # Save the best model\n# file_pairs, num_classes = prepare_file_pairs(_input_folder)\n# best_model = timm.create_model('resnext50_32x4d', pretrained=True)\n# best_model.fc = nn.Linear(best_model.fc.in_features, num_classes)\n# best_model.load_state_dict(torch.load(f\"{_output_model_folder}/model_best_resnext_trial_{trial.number}.pth\"))\n# best_model_save_path = f\"{_output_model_folder}/model_best_resnext.pth\"\n# torch.save(best_model.state_dict(), best_model_save_path)\n\n# log_message(f\"Best model saved to {best_model_save_path}\")\n\n# # Load and test the best model\n# best_model.load_state_dict(torch.load(best_model_save_path))\n# best_model.eval()\n# best_model.to(device)\n\n# test_transformation = transforms.Compose([\n#     transforms.Resize((224, 224)),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n# ])\n\n# test_file_pairs, _ = prepare_file_pairs(_test_folder)\n# test_dataset = CustomDataset(test_file_pairs, transform=test_transformation)\n# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n# submission = []\n\n# with torch.no_grad():\n#     for idx, (inputs, _) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n#         inputs = inputs.to(device)\n#         outputs = best_model(inputs)\n#         probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy().flatten()\n        \n#         # Generate row_id\n#         row_id = os.path.basename(test_file_pairs[idx][0]).replace('.ogg', '')\n        \n#         submission.append([row_id] + probs.tolist())\n\n# # Create submission DataFrame\n# column_names = ['row_id'] + [f'label_{i}' for i in range(num_classes)]\n# submission_df = pd.DataFrame(submission, columns=column_names)\n\n# # Save to CSV\n# submission_csv_path = f\"{_output_submission_folder}/Submission.csv\"\n# submission_df.to_csv(submission_csv_path, index=False)\n# log_message(f\"Submission file saved to {submission_csv_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T14:10:04.980024Z","iopub.execute_input":"2024-07-16T14:10:04.980720Z","iopub.status.idle":"2024-07-16T14:10:05.000817Z","shell.execute_reply.started":"2024-07-16T14:10:04.980685Z","shell.execute_reply":"2024-07-16T14:10:04.999442Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nfrom io import BytesIO\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport timm\nimport librosa\nimport librosa.display\nimport noisereduce as nr\nimport numpy as np\n\n_test_folder = \"/kaggle/input/birdclef-2024/test_soundscapes\"\n_output_submission_folder = \"/kaggle/working\"\n_sample_rate = 16000\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef load_audio(filename):\n    audio, sr = librosa.load(filename, sr=_sample_rate)\n    return audio, sr\n\ndef segment_audio(segment, segment_length=5, sr=_sample_rate):\n    segmented_chunks = []\n    samples_per_segment = segment_length * sr\n    for start in range(0, len(segment), samples_per_segment):\n        end = start + samples_per_segment\n        segmented_chunks.append(segment[start:end])\n    return segmented_chunks\n\ndef generate_square_spectrogram(audio, sr, size=224, fmin=2000, fmax=8000):\n    s = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=fmax, fmin=fmin)\n    s_dB = librosa.power_to_db(s, ref=np.max)\n    fig, ax = plt.subplots(figsize=(size / 100, size / 100), dpi=100)\n    img = librosa.display.specshow(s_dB, sr=sr, x_axis='time', y_axis='mel', fmin=fmin, fmax=fmax, cmap='gray', ax=ax)\n    ax.axis('off')\n    plt.tight_layout(pad=0)\n    buf = BytesIO()\n    fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n    plt.close(fig)\n    buf.seek(0)\n    image = Image.open(buf).convert('RGB')\n    return image\n\nclass CustomDataset(Dataset):\n    def __init__(self, file_pairs, transform=None):\n        self.file_pairs = file_pairs\n        self.transform = transform\n\n    def __len__(self):\n        return sum(len(segment) for _, _, segment in self.file_pairs)\n\n    def __getitem__(self, idx):\n        current_idx = 0\n        for input_file_path, label, segments in self.file_pairs:\n            if idx < current_idx + len(segments):\n                segment = segments[idx - current_idx]\n                image = generate_square_spectrogram(segment, _sample_rate)\n                if self.transform:\n                    image = self.transform(image)\n                return image, label\n            current_idx += len(segments)\n        raise IndexError(\"Index out of range\")\n\ndef prepare_file_pairs(input_folder):\n    file_pairs = []\n    input_folder = Path(input_folder)\n    class_names = sorted([f.name for f in input_folder.iterdir() if f.is_dir()])\n    class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n    \n    for input_path in tqdm(input_folder.rglob('*.ogg'), desc=\"Preparing file pairs\"):\n        label = class_to_idx[input_path.parent.name]\n        audio, sr = load_audio(input_path)\n        audio = nr.reduce_noise(audio, sr)\n        segments = segment_audio(audio, segment_length=5, sr=sr)\n        file_pairs.append((input_path, label, segments))\n    return file_pairs, len(class_names)\n\n# Define the number of classes manually or from the training data\nnum_classes = 182  # Replace with the actual number of classes\n\n# Load and test the best model\nmodel_path = '/kaggle/input/resnext_best/pytorch/resnext_best/1/model_best_resnext.pth'\nbest_model = timm.create_model('resnext50_32x4d', pretrained=True)\nbest_model.fc = nn.Linear(best_model.fc.in_features, num_classes)\nbest_model.load_state_dict(torch.load(model_path))\nbest_model.eval()\nbest_model.to(device)\n\ntest_transformation = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_file_pairs, _ = prepare_file_pairs(_test_folder)\ntest_dataset = CustomDataset(test_file_pairs, transform=test_transformation)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\nsubmission = []\n\nwith torch.no_grad():\n    for idx, (inputs, _) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n        inputs = inputs.to(device)\n        outputs = best_model(inputs)\n        probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy().flatten()\n        \n        # Generate row_id\n        row_id = os.path.basename(test_file_pairs[idx][0]).replace('.ogg', '')\n        \n        submission.append([row_id] + probs.tolist())\n\n# Create submission DataFrame\ncolumn_names = ['row_id'] + [f'label_{i}' for i in range(num_classes)]\nsubmission_df = pd.DataFrame(submission, columns=column_names)\n\n# Save to CSV\nsubmission_csv_path = f\"{_output_submission_folder}/Submission.csv\"\nsubmission_df.to_csv(submission_csv_path, index=False)\nprint(f\"Submission file saved to {submission_csv_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T14:27:33.169728Z","iopub.execute_input":"2024-07-16T14:27:33.170278Z","iopub.status.idle":"2024-07-16T14:27:34.259550Z","shell.execute_reply.started":"2024-07-16T14:27:33.170235Z","shell.execute_reply":"2024-07-16T14:27:34.257830Z"},"trusted":true},"execution_count":22,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m best_model \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mcreate_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnext50_32x4d\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     88\u001b[0m best_model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(best_model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features, num_classes)\n\u001b[0;32m---> 89\u001b[0m best_model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     90\u001b[0m best_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     91\u001b[0m best_model\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1391\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1392\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:1366\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1366\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1367\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1368\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1371\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:381\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 381\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:274\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 274\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    276\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:258\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    255\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    259\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    260\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    261\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    262\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    263\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n","\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."],"ename":"RuntimeError","evalue":"Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.","output_type":"error"}]},{"cell_type":"code","source":"# import os\n\n# def print_directory_structure(root_dir, indent=\"\"):\n#     for item in os.listdir(root_dir):\n#         item_path = os.path.join(root_dir, item)\n#         if os.path.isdir(item_path):\n#             print(indent + \"|-- \" + item)\n#             print_directory_structure(item_path, indent + \"    \")\n#         elif not item.endswith('.ogg'):\n#             print(indent + \"|-- \" + item)\n\n# # Change '/kaggle/input' to the root directory of your Kaggle project folder\n# root_directory = '/kaggle/input'\n# print_directory_structure(root_directory)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T14:18:34.690726Z","iopub.execute_input":"2024-07-16T14:18:34.691266Z","iopub.status.idle":"2024-07-16T14:19:27.641010Z","shell.execute_reply.started":"2024-07-16T14:18:34.691227Z","shell.execute_reply":"2024-07-16T14:19:27.639673Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"|-- birdclef-2024\n    |-- sample_submission.csv\n    |-- unlabeled_soundscapes\n    |-- train_audio\n        |-- redspu1\n        |-- zitcis1\n        |-- marsan\n        |-- rewlap1\n        |-- grenig1\n        |-- insbab1\n        |-- commyn\n        |-- labcro1\n        |-- yebbul3\n        |-- whtkin2\n        |-- brasta1\n        |-- brnhao1\n        |-- bkcbul1\n        |-- grywag\n        |-- graher1\n        |-- brodro1\n        |-- purher1\n        |-- bkskit1\n        |-- eurbla2\n        |-- grtdro1\n        |-- comtai1\n        |-- paisto1\n        |-- brcful1\n        |-- grehor1\n        |-- whcbar1\n        |-- blakit1\n        |-- integr\n        |-- gryfra\n        |-- grnsan\n        |-- whbsho3\n        |-- sqtbul1\n        |-- greegr\n        |-- wemhar1\n        |-- comsan\n        |-- rewbul\n        |-- cohcuc1\n        |-- heswoo1\n        |-- gybpri1\n        |-- gargan\n        |-- darter2\n        |-- rerswa1\n        |-- grnwar1\n        |-- isbduc1\n        |-- brwjac1\n        |-- mawthr1\n        |-- eaywag1\n        |-- pursun3\n        |-- lblwar1\n        |-- kerlau2\n        |-- junmyn1\n        |-- rutfly6\n        |-- oripip1\n        |-- houspa\n        |-- lobsun2\n        |-- wynlau1\n        |-- eucdov\n        |-- compea\n        |-- spodov\n        |-- nutman\n        |-- revbul\n        |-- aspswi1\n        |-- blaeag1\n        |-- whiter2\n        |-- litspi1\n        |-- litswi1\n        |-- rufwoo2\n        |-- rossta2\n        |-- barfly1\n        |-- ruftre2\n        |-- inpher1\n        |-- maghor2\n        |-- litgre1\n        |-- grynig2\n        |-- lirplo\n        |-- rorpar\n        |-- gloibi\n        |-- comkin1\n        |-- orihob2\n        |-- sohmyn1\n        |-- yebbab1\n        |-- plapri1\n        |-- stbkin1\n        |-- blhori1\n        |-- comros\n        |-- ashdro1\n        |-- dafbab1\n        |-- ingori1\n        |-- rocpig\n        |-- jerbus2\n        |-- inbrob1\n        |-- houcro1\n        |-- asbfly\n        |-- whbwat1\n        |-- vefnut1\n        |-- emedov2\n        |-- asikoe2\n        |-- bcnher\n        |-- woosan\n        |-- gyhcaf1\n        |-- tilwar1\n        |-- scamin3\n        |-- blrwar1\n        |-- blnmon1\n        |-- indrob1\n        |-- sbeowl1\n        |-- hoopoe\n        |-- lesyel1\n        |-- pabflo1\n        |-- comgre\n        |-- kenplo1\n        |-- brfowl1\n        |-- insowl1\n        |-- whbtre1\n        |-- pomgrp2\n        |-- nilfly2\n        |-- malwoo1\n        |-- crseag1\n        |-- spepic1\n        |-- eurcoo\n        |-- copbar1\n        |-- crfbar1\n        |-- thbwar1\n        |-- plhpar1\n        |-- grewar3\n        |-- aspfly1\n        |-- plaflo1\n        |-- indtit1\n        |-- forwag1\n        |-- junbab2\n        |-- commoo3\n        |-- crbsun2\n        |-- comior1\n        |-- whbbul2\n        |-- piebus1\n        |-- grejun2\n        |-- brnshr\n        |-- lewduc1\n        |-- whrmun\n        |-- spoowl1\n        |-- bladro1\n        |-- ashpri1\n        |-- sttwoo1\n        |-- chbeat1\n        |-- grefla1\n        |-- grbeat1\n        |-- tibfly3\n        |-- junowl1\n        |-- brakit1\n        |-- ashwoo2\n        |-- whbwag1\n        |-- whbwoo2\n        |-- niwpig1\n        |-- maltro1\n        |-- bwfshr1\n        |-- asiope1\n        |-- bncwoo3\n        |-- goflea1\n        |-- vehpar1\n        |-- malpar1\n        |-- rufbab3\n        |-- placuc3\n        |-- indrol2\n        |-- purswa3\n        |-- pursun4\n        |-- cregos1\n        |-- bkrfla1\n        |-- grecou1\n        |-- piekin1\n        |-- brwowl1\n        |-- btbeat1\n        |-- laudov1\n        |-- wbbfly1\n        |-- putbab1\n        |-- moipig1\n        |-- shikra1\n        |-- comfla1\n        |-- barswa\n        |-- categr\n        |-- smamin1\n        |-- indpit1\n        |-- bkwsti\n        |-- litegr\n    |-- eBird_Taxonomy_v2021.csv\n    |-- train_metadata.csv\n    |-- test_soundscapes\n        |-- readme.txt\n|-- resnext_best\n    |-- pytorch\n        |-- resnext_best\n            |-- 1\n                |-- model_best_resnext.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"# import os\n\n# train_audio_folder = '/kaggle/input/birdclef-2024/train_audio'\n\n# def count_folders(directory):\n#     return sum(1 for entry in os.scandir(directory) if entry.is_dir())\n\n# folder_count = count_folders(train_audio_folder)\n# print(f\"The number of folders in '{train_audio_folder}' is: {folder_count}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T14:26:17.152836Z","iopub.execute_input":"2024-07-16T14:26:17.154387Z","iopub.status.idle":"2024-07-16T14:26:17.163200Z","shell.execute_reply.started":"2024-07-16T14:26:17.154335Z","shell.execute_reply":"2024-07-16T14:26:17.161459Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"The number of folders in '/kaggle/input/birdclef-2024/train_audio' is: 182\n","output_type":"stream"}]}]}