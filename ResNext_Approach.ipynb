{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bbed7c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-16T14:34:09.524134Z",
     "iopub.status.busy": "2024-07-16T14:34:09.523731Z",
     "iopub.status.idle": "2024-07-16T14:34:10.615646Z",
     "shell.execute_reply": "2024-07-16T14:34:10.614228Z"
    },
    "papermill": {
     "duration": 1.100359,
     "end_time": "2024-07-16T14:34:10.618355",
     "exception": false,
     "start_time": "2024-07-16T14:34:09.517996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -r /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15713d36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-16T14:34:10.628735Z",
     "iopub.status.busy": "2024-07-16T14:34:10.627881Z",
     "iopub.status.idle": "2024-07-16T14:34:27.474546Z",
     "shell.execute_reply": "2024-07-16T14:34:27.473371Z"
    },
    "papermill": {
     "duration": 16.854633,
     "end_time": "2024-07-16T14:34:27.477347",
     "exception": false,
     "start_time": "2024-07-16T14:34:10.622714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\r\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.32.3)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abee5ec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-16T14:34:27.487521Z",
     "iopub.status.busy": "2024-07-16T14:34:27.487184Z",
     "iopub.status.idle": "2024-07-16T14:34:45.176339Z",
     "shell.execute_reply": "2024-07-16T14:34:45.175098Z"
    },
    "papermill": {
     "duration": 17.697325,
     "end_time": "2024-07-16T14:34:45.179092",
     "exception": false,
     "start_time": "2024-07-16T14:34:27.481767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting noisereduce\r\n",
      "  Downloading noisereduce-3.0.2-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\r\n",
      "Requirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (0.10.2.post1)\r\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (3.6.1)\r\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (1.0.3)\r\n",
      "Collecting scikit-metrics\r\n",
      "  Downloading scikit-metrics-0.1.0.tar.gz (3.9 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.1)\r\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (9.5.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from noisereduce) (1.11.4)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from noisereduce) (3.7.5)\r\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa) (3.0.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.4.2)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (5.1.1)\r\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.58.1)\r\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.12.1)\r\n",
      "Requirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.8.1)\r\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.3.7)\r\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (4.9.0)\r\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.3)\r\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.0.7)\r\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.1)\r\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.25)\r\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.1)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from timm) (2.1.2)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.2)\r\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.23.2)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\r\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\r\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.41.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.1.1)\r\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (3.11.0)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.32.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\r\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.3.1)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->noisereduce) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->noisereduce) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->noisereduce) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->noisereduce) (1.4.5)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->timm) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.1.2)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\r\n",
      "Downloading noisereduce-3.0.2-py3-none-any.whl (22 kB)\r\n",
      "Building wheels for collected packages: scikit-metrics\r\n",
      "  Building wheel for scikit-metrics (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for scikit-metrics: filename=scikit_metrics-0.1.0-py3-none-any.whl size=4394 sha256=743ca0db84b1d8c8b3699b506e65cb2ec05c4e2f9da6838934a2e5fcb4d9e8ad\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/45/a3/9b/8f375bbf235b0cfbee9a841c94b4bbfb13fad10db9cab4d5d4\r\n",
      "Successfully built scikit-metrics\r\n",
      "Installing collected packages: scikit-metrics, noisereduce\r\n",
      "Successfully installed noisereduce-3.0.2 scikit-metrics-0.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install noisereduce numpy tqdm librosa optuna timm scikit-metrics pandas pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a45613",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-16T14:34:45.193789Z",
     "iopub.status.busy": "2024-07-16T14:34:45.193445Z",
     "iopub.status.idle": "2024-07-16T14:34:45.199037Z",
     "shell.execute_reply": "2024-07-16T14:34:45.198137Z"
    },
    "papermill": {
     "duration": 0.015219,
     "end_time": "2024-07-16T14:34:45.201102",
     "exception": false,
     "start_time": "2024-07-16T14:34:45.185883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This create a small subset for testing\n",
    "\n",
    "# import os\n",
    "# import shutil\n",
    "# import sys\n",
    "\n",
    "# def copy_files(src_root, dest_root):\n",
    "#     # Create the destination root directory if it doesn't exist\n",
    "#     os.makedirs(dest_root, exist_ok=True)\n",
    "\n",
    "#     # Get the first 10 subdirectories\n",
    "#     subdirs = [os.path.join(src_root, d) for d in os.listdir(src_root) if os.path.isdir(os.path.join(src_root, d))]\n",
    "#     subdirs = subdirs[:5]\n",
    "\n",
    "#     for subdir in subdirs:\n",
    "#         # Get the first 5 files in the current subdirectory\n",
    "#         files = [os.path.join(subdir, f) for f in os.listdir(subdir) if os.path.isfile(os.path.join(subdir, f))]\n",
    "#         files = files[:5]\n",
    "\n",
    "#         # Create the corresponding subdirectory in the destination\n",
    "#         dest_subdir = os.path.join(dest_root, os.path.basename(subdir))\n",
    "#         os.makedirs(dest_subdir, exist_ok=True)\n",
    "\n",
    "#         for file in files:\n",
    "#             shutil.copy(file, dest_subdir)\n",
    "    \n",
    "#     print(\"Copying completed.\")\n",
    "\n",
    "# src_root = '/kaggle/input/birdclef-2024/train_audio'\n",
    "# dest_root = '/kaggle/working/train_audio_subset'\n",
    "\n",
    "# copy_files(src_root, dest_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e26303",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-16T14:34:45.215459Z",
     "iopub.status.busy": "2024-07-16T14:34:45.215143Z",
     "iopub.status.idle": "2024-07-16T14:34:45.230354Z",
     "shell.execute_reply": "2024-07-16T14:34:45.229423Z"
    },
    "papermill": {
     "duration": 0.025044,
     "end_time": "2024-07-16T14:34:45.232462",
     "exception": false,
     "start_time": "2024-07-16T14:34:45.207418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# import os\n",
    "# import time\n",
    "# from pathlib import Path\n",
    "# from io import BytesIO\n",
    "# from PIL import Image\n",
    "# import pandas as pd\n",
    "\n",
    "# import librosa\n",
    "# import matplotlib.pyplot as plt\n",
    "# import noisereduce as nr\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torchvision import transforms, datasets\n",
    "# from torch.utils.data import DataLoader, random_split\n",
    "# import timm\n",
    "# import optuna\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# _input_folder = \"/kaggle/input/birdclef-2024/train_audio\"\n",
    "# # _input_folder = '/kaggle/working/train_audio_subset' # For testing only\n",
    "# _test_folder = \"/kaggle/input/birdclef-2024/test_soundscapes\"\n",
    "# _output_model_folder = \"/kaggle/working/models\"\n",
    "# _output_log_folder = \"/kaggle/working/logs\"\n",
    "# _output_submission_folder = \"/kaggle/working\"\n",
    "# _sample_rate = 16000\n",
    "# _n_epochs = 5\n",
    "# _n_optuna_trials = 3\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# def load_audio(filename):\n",
    "#     audio, sr = librosa.load(filename, sr=_sample_rate)\n",
    "#     return audio, sr\n",
    "\n",
    "# def segment_audio(segment, segment_length=5, sr=_sample_rate):\n",
    "#     segmented_chunks = []\n",
    "#     samples_per_segment = segment_length * sr\n",
    "#     for start in range(0, len(segment), samples_per_segment):\n",
    "#         end = start + samples_per_segment\n",
    "#         segmented_chunks.append(segment[start:end])\n",
    "#     return segmented_chunks\n",
    "\n",
    "# def generate_square_spectrogram(audio, sr, size=224, fmin=2000, fmax=8000):\n",
    "#     s = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=fmax, fmin=fmin)\n",
    "#     s_dB = librosa.power_to_db(s, ref=np.max)\n",
    "#     fig, ax = plt.subplots(figsize=(size / 100, size / 100), dpi=100)\n",
    "#     img = librosa.display.specshow(s_dB, sr=sr, x_axis='time', y_axis='mel', fmin=fmin, fmax=fmax, cmap='gray', ax=ax)\n",
    "#     ax.axis('off')\n",
    "#     plt.tight_layout(pad=0)\n",
    "#     buf = BytesIO()\n",
    "#     fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n",
    "#     plt.close(fig)\n",
    "#     buf.seek(0)\n",
    "#     image = Image.open(buf).convert('RGB')\n",
    "#     return image\n",
    "\n",
    "# class CustomDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, file_pairs, transform=None):\n",
    "#         self.file_pairs = file_pairs\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return sum(len(segment) for _, _, segment in self.file_pairs)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         current_idx = 0\n",
    "#         for input_file_path, label, segments in self.file_pairs:\n",
    "#             if idx < current_idx + len(segments):\n",
    "#                 segment = segments[idx - current_idx]\n",
    "#                 image = generate_square_spectrogram(segment, _sample_rate)\n",
    "#                 if self.transform:\n",
    "#                     image = self.transform(image)\n",
    "#                 return image, label\n",
    "#             current_idx += len(segments)\n",
    "#         raise IndexError(\"Index out of range\")\n",
    "\n",
    "# def prepare_file_pairs(input_folder):\n",
    "#     file_pairs = []\n",
    "#     input_folder = Path(input_folder)\n",
    "#     class_names = sorted([f.name for f in input_folder.iterdir() if f.is_dir()])\n",
    "#     class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "    \n",
    "#     for input_path in input_folder.rglob('*.ogg'):\n",
    "#         label = class_to_idx[input_path.parent.name]\n",
    "#         audio, sr = load_audio(input_path)\n",
    "#         audio = nr.reduce_noise(audio, sr)\n",
    "#         segments = segment_audio(audio, segment_length=5, sr=sr)\n",
    "#         file_pairs.append((input_path, label, segments))\n",
    "#     return file_pairs, len(class_names)\n",
    "\n",
    "# def objective(trial):\n",
    "#     model = timm.create_model('resnext50_32x4d', pretrained=True)\n",
    "\n",
    "#     transformation = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "\n",
    "#     file_pairs, num_classes = prepare_file_pairs(_input_folder)\n",
    "#     dataset = CustomDataset(file_pairs, transform=transformation)\n",
    "\n",
    "#     train_size = int(0.8 * len(dataset))\n",
    "#     val_size = len(dataset) - train_size\n",
    "#     train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "#     batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "#     model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "#     model.to(device)\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=trial.suggest_loguniform(\"lr\", 1e-5, 1e-1))\n",
    "\n",
    "#     train_losses = []\n",
    "#     val_losses = []\n",
    "#     train_accuracies = []\n",
    "#     val_accuracies = []\n",
    "#     val_roc_aucs = []\n",
    "\n",
    "#     for epoch in range(_n_epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         correct_train = 0\n",
    "#         total_train = 0\n",
    "\n",
    "#         train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{_n_epochs}\")\n",
    "\n",
    "#         for inputs, labels in train_loader_tqdm:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total_train += labels.size(0)\n",
    "#             correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "#             train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "#         epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#         epoch_accuracy = 100 * correct_train / total_train\n",
    "#         train_losses.append(epoch_loss)\n",
    "#         train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "#         val_loss = 0.0\n",
    "#         correct_val = 0\n",
    "#         total_val = 0\n",
    "#         all_labels = []\n",
    "#         all_probs = []\n",
    "\n",
    "#         model.eval()\n",
    "#         val_loader_tqdm = tqdm(val_loader, desc=\"Validating\")\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in val_loader_tqdm:\n",
    "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 val_loss += loss.item() * inputs.size(0)\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 total_val += labels.size(0)\n",
    "#                 correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "#                 all_labels.extend(labels.cpu().numpy())\n",
    "#                 all_probs.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "#                 val_loader_tqdm.set_postfix(val_loss=loss.item())\n",
    "\n",
    "#         epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "#         epoch_val_accuracy = 100 * correct_val / total_val\n",
    "#         val_losses.append(epoch_val_loss)\n",
    "#         val_accuracies.append(epoch_val_accuracy)\n",
    "\n",
    "#         roc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
    "#         val_roc_aucs.append(roc_auc)\n",
    "\n",
    "#         trial.report(epoch_val_accuracy, epoch)\n",
    "\n",
    "#         if trial.should_prune():\n",
    "#             raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "#     # Save the model after tuning\n",
    "#     os.makedirs(_output_model_folder, exist_ok=True)\n",
    "#     model_save_path = f\"{_output_model_folder}/model_best_resnext_trial_{trial.number}.pth\"\n",
    "#     torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "#     return val_accuracies[-1]\n",
    "\n",
    "# # Set up logging\n",
    "# os.makedirs(_output_log_folder, exist_ok=True)\n",
    "# log_file = f\"{_output_log_folder}/training_log.txt\"\n",
    "\n",
    "# def log_message(message):\n",
    "#     print(message)\n",
    "#     with open(log_file, 'a') as f:\n",
    "#         f.write(message + '\\n')\n",
    "\n",
    "# # Set up the Optuna study\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=_n_optuna_trials, timeout=600)\n",
    "\n",
    "# log_message(f\"Number of finished trials: {len(study.trials)}\")\n",
    "# log_message(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "\n",
    "# log_message(f\"  Value: {trial.value}\")\n",
    "# log_message(\"  Params: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     log_message(f\"    {key}: {value}\")\n",
    "\n",
    "# # Save the best model\n",
    "# file_pairs, num_classes = prepare_file_pairs(_input_folder)\n",
    "# best_model = timm.create_model('resnext50_32x4d', pretrained=True)\n",
    "# best_model.fc = nn.Linear(best_model.fc.in_features, num_classes)\n",
    "# best_model.load_state_dict(torch.load(f\"{_output_model_folder}/model_best_resnext_trial_{trial.number}.pth\"))\n",
    "# best_model_save_path = f\"{_output_model_folder}/model_best_resnext.pth\"\n",
    "# torch.save(best_model.state_dict(), best_model_save_path)\n",
    "\n",
    "# log_message(f\"Best model saved to {best_model_save_path}\")\n",
    "\n",
    "# # Load and test the best model\n",
    "# best_model.load_state_dict(torch.load(best_model_save_path))\n",
    "# best_model.eval()\n",
    "# best_model.to(device)\n",
    "\n",
    "# test_transformation = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# test_file_pairs, _ = prepare_file_pairs(_test_folder)\n",
    "# test_dataset = CustomDataset(test_file_pairs, transform=test_transformation)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# submission = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for idx, (inputs, _) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
    "#         inputs = inputs.to(device)\n",
    "#         outputs = best_model(inputs)\n",
    "#         probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy().flatten()\n",
    "        \n",
    "#         # Generate row_id\n",
    "#         row_id = os.path.basename(test_file_pairs[idx][0]).replace('.ogg', '')\n",
    "        \n",
    "#         submission.append([row_id] + probs.tolist())\n",
    "\n",
    "# # Create submission DataFrame\n",
    "# column_names = ['row_id'] + [f'label_{i}' for i in range(num_classes)]\n",
    "# submission_df = pd.DataFrame(submission, columns=column_names)\n",
    "\n",
    "# # Save to CSV\n",
    "# submission_csv_path = f\"{_output_submission_folder}/Submission.csv\"\n",
    "# submission_df.to_csv(submission_csv_path, index=False)\n",
    "# log_message(f\"Submission file saved to {submission_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "929a97c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-16T14:34:45.246951Z",
     "iopub.status.busy": "2024-07-16T14:34:45.246640Z",
     "iopub.status.idle": "2024-07-16T14:35:09.577670Z",
     "shell.execute_reply": "2024-07-16T14:35:09.576658Z"
    },
    "papermill": {
     "duration": 24.341173,
     "end_time": "2024-07-16T14:35:09.580234",
     "exception": false,
     "start_time": "2024-07-16T14:34:45.239061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2ea2f20987404fac383bb0ba4658d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/100M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing file pairs: 0it [00:00, ?it/s]\n",
      "Testing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to /kaggle/working/Submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "import librosa\n",
    "import librosa.display\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "_test_folder = \"/kaggle/input/birdclef-2024/test_soundscapes\"\n",
    "_output_submission_folder = \"/kaggle/working\"\n",
    "_sample_rate = 16000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_audio(filename):\n",
    "    audio, sr = librosa.load(filename, sr=_sample_rate)\n",
    "    return audio, sr\n",
    "\n",
    "def segment_audio(segment, segment_length=5, sr=_sample_rate):\n",
    "    segmented_chunks = []\n",
    "    samples_per_segment = segment_length * sr\n",
    "    for start in range(0, len(segment), samples_per_segment):\n",
    "        end = start + samples_per_segment\n",
    "        segmented_chunks.append(segment[start:end])\n",
    "    return segmented_chunks\n",
    "\n",
    "def generate_square_spectrogram(audio, sr, size=224, fmin=2000, fmax=8000):\n",
    "    s = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=fmax, fmin=fmin)\n",
    "    s_dB = librosa.power_to_db(s, ref=np.max)\n",
    "    fig, ax = plt.subplots(figsize=(size / 100, size / 100), dpi=100)\n",
    "    img = librosa.display.specshow(s_dB, sr=sr, x_axis='time', y_axis='mel', fmin=fmin, fmax=fmax, cmap='gray', ax=ax)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    buf = BytesIO()\n",
    "    fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    image = Image.open(buf).convert('RGB')\n",
    "    return image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_pairs, transform=None):\n",
    "        self.file_pairs = file_pairs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(segment) for _, _, segment in self.file_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_idx = 0\n",
    "        for input_file_path, label, segments in self.file_pairs:\n",
    "            if idx < current_idx + len(segments):\n",
    "                segment = segments[idx - current_idx]\n",
    "                image = generate_square_spectrogram(segment, _sample_rate)\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                return image, label\n",
    "            current_idx += len(segments)\n",
    "        raise IndexError(\"Index out of range\")\n",
    "\n",
    "def prepare_file_pairs(input_folder):\n",
    "    file_pairs = []\n",
    "    input_folder = Path(input_folder)\n",
    "    class_names = sorted([f.name for f in input_folder.iterdir() if f.is_dir()])\n",
    "    class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "    \n",
    "    for input_path in tqdm(input_folder.rglob('*.ogg'), desc=\"Preparing file pairs\"):\n",
    "        label = class_to_idx[input_path.parent.name]\n",
    "        audio, sr = load_audio(input_path)\n",
    "        audio = nr.reduce_noise(audio, sr)\n",
    "        segments = segment_audio(audio, segment_length=5, sr=sr)\n",
    "        file_pairs.append((input_path, label, segments))\n",
    "    return file_pairs, len(class_names)\n",
    "\n",
    "# Define the number of classes manually or from the training data\n",
    "num_classes = 182  # Replace with the actual number of classes\n",
    "\n",
    "# Load and test the best model\n",
    "model_path = '/kaggle/input/resnext_best/pytorch/resnext_best/1/model_best_resnext.pth'\n",
    "best_model = timm.create_model('resnext50_32x4d', pretrained=True)\n",
    "best_model.fc = nn.Linear(best_model.fc.in_features, num_classes)\n",
    "best_model.load_state_dict(torch.load(model_path))\n",
    "best_model.eval()\n",
    "best_model.to(device)\n",
    "\n",
    "test_transformation = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_file_pairs, _ = prepare_file_pairs(_test_folder)\n",
    "test_dataset = CustomDataset(test_file_pairs, transform=test_transformation)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "submission = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (inputs, _) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = best_model(inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy().flatten()\n",
    "        \n",
    "        # Generate row_id\n",
    "        row_id = os.path.basename(test_file_pairs[idx][0]).replace('.ogg', '')\n",
    "        \n",
    "        submission.append([row_id] + probs.tolist())\n",
    "\n",
    "# Create submission DataFrame\n",
    "column_names = ['row_id'] + [f'label_{i}' for i in range(num_classes)]\n",
    "submission_df = pd.DataFrame(submission, columns=column_names)\n",
    "\n",
    "# Save to CSV\n",
    "submission_csv_path = f\"{_output_submission_folder}/Submission.csv\"\n",
    "submission_df.to_csv(submission_csv_path, index=False)\n",
    "print(f\"Submission file saved to {submission_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfaf2969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-16T14:35:09.595609Z",
     "iopub.status.busy": "2024-07-16T14:35:09.595314Z",
     "iopub.status.idle": "2024-07-16T14:35:09.599562Z",
     "shell.execute_reply": "2024-07-16T14:35:09.598742Z"
    },
    "papermill": {
     "duration": 0.014226,
     "end_time": "2024-07-16T14:35:09.601519",
     "exception": false,
     "start_time": "2024-07-16T14:35:09.587293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def print_directory_structure(root_dir, indent=\"\"):\n",
    "#     for item in os.listdir(root_dir):\n",
    "#         item_path = os.path.join(root_dir, item)\n",
    "#         if os.path.isdir(item_path):\n",
    "#             print(indent + \"|-- \" + item)\n",
    "#             print_directory_structure(item_path, indent + \"    \")\n",
    "#         elif not item.endswith('.ogg'):\n",
    "#             print(indent + \"|-- \" + item)\n",
    "\n",
    "# # Change '/kaggle/input' to the root directory of your Kaggle project folder\n",
    "# root_directory = '/kaggle/input'\n",
    "# print_directory_structure(root_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a9091f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-16T14:35:09.616951Z",
     "iopub.status.busy": "2024-07-16T14:35:09.616642Z",
     "iopub.status.idle": "2024-07-16T14:35:09.620544Z",
     "shell.execute_reply": "2024-07-16T14:35:09.619644Z"
    },
    "papermill": {
     "duration": 0.013964,
     "end_time": "2024-07-16T14:35:09.622591",
     "exception": false,
     "start_time": "2024-07-16T14:35:09.608627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# train_audio_folder = '/kaggle/input/birdclef-2024/train_audio'\n",
    "\n",
    "# def count_folders(directory):\n",
    "#     return sum(1 for entry in os.scandir(directory) if entry.is_dir())\n",
    "\n",
    "# folder_count = count_folders(train_audio_folder)\n",
    "# print(f\"The number of folders in '{train_audio_folder}' is: {folder_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8068726,
     "sourceId": 70203,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 65558,
     "sourceId": 77979,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 66.554372,
   "end_time": "2024-07-16T14:35:12.684863",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-16T14:34:06.130491",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "39c8244cf2d2448980ed7ac3ccda52cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3c2407e9f6d94d54b59f5067c5946e7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_84c58bd54df94690b061aac0b9c54e0f",
       "placeholder": "​",
       "style": "IPY_MODEL_8d959019c0b04d91a3654da6a0d38672",
       "value": " 100M/100M [00:05&lt;00:00, 21.5MB/s]"
      }
     },
     "843d9dd212e74467b7c09b01a4d9381e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_86a01959f6cf406d816eb91dbebc2ba8",
       "max": 100417784.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f3f6b9a538ab4d4d97be6abf996e89d5",
       "value": 100417784.0
      }
     },
     "84c58bd54df94690b061aac0b9c54e0f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "86a01959f6cf406d816eb91dbebc2ba8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d959019c0b04d91a3654da6a0d38672": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "af3b60269cd640fc8c9c1f658972fabf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cc2ea2f20987404fac383bb0ba4658d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d62fc532556d4731a13a4a477a19ddb4",
        "IPY_MODEL_843d9dd212e74467b7c09b01a4d9381e",
        "IPY_MODEL_3c2407e9f6d94d54b59f5067c5946e7d"
       ],
       "layout": "IPY_MODEL_ea2f0ef5cf964ed1beb33edf86e45147"
      }
     },
     "d62fc532556d4731a13a4a477a19ddb4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_39c8244cf2d2448980ed7ac3ccda52cd",
       "placeholder": "​",
       "style": "IPY_MODEL_af3b60269cd640fc8c9c1f658972fabf",
       "value": "model.safetensors: 100%"
      }
     },
     "ea2f0ef5cf964ed1beb33edf86e45147": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f3f6b9a538ab4d4d97be6abf996e89d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
